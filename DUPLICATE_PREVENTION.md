# Duplicate Prevention System

## Overview

The scraper automatically prevents duplicate agents across multiple scraping sessions using a persistent history file.

---

## How It Works

### Within a Single Run
‚úÖ **Automatic** - No configuration needed
- Duplicates removed by `agent_id`
- Happens automatically during every search
- Based on agents returned from Zillow API

### Across Multiple Runs
‚úÖ **Automatic by default** - Can be disabled if needed
- Tracks all scraped agent IDs in `~/.agentharvest_history.json`
- Automatically skips agents from previous runs
- Works across days, weeks, months
- Persistent until manually cleared

---

## Usage Examples

### Example 1: Daily Scraping (Recommended)

```python
from agentharvest import scrape_agent

# Day 1: Scrape 500 agents
agents_day1 = scrape_agent(state="California", limit=500, fetch_profiles=True)
# Result: 500 new agents scraped
# History: 500 agent IDs saved

# Day 2: Scrape 500 more agents
agents_day2 = scrape_agent(state="California", limit=500, fetch_profiles=True)
# Result: 500 NEW agents (excluding the 500 from Day 1)
# History: 1000 agent IDs saved

# Day 3: Continue...
agents_day3 = scrape_agent(state="California", limit=500, fetch_profiles=True)
# Result: 500 NEW agents (excluding previous 1000)
# History: 1500 agent IDs saved
```

**Total:** 1500 unique agents, zero duplicates!

### Example 2: Re-scraping Same Agents

```python
# Disable duplicate prevention to allow re-scraping
agents = scrape_agent(
    state="Nevada",
    limit=100,
    fetch_profiles=True,
    skip_scraped=False  # Allow re-scraping
)
```

### Example 3: Clearing History

```python
from agentharvest import clear_scraping_history, scrape_agent

# Clear all history to start fresh
clear_scraping_history()

# Now all agents can be scraped again
agents = scrape_agent(state="Texas", limit=200)
```

---

## Streamlit UI Usage

### Duplicate Prevention (Enabled by Default)

1. Open Streamlit: `streamlit run app.py`
2. Expand **‚öôÔ∏è Advanced Options** in sidebar
3. Find **üîÑ Duplicate Prevention** section
4. **"Skip Previously Scraped Agents"** is checked ‚úÖ by default

**Behavior:**
- ‚úÖ First run: Scrapes all agents found
- ‚úÖ Second run: Skips agents from first run
- ‚úÖ Third run: Skips agents from first + second run
- ‚úÖ And so on...

### To Allow Re-Scraping

1. **Uncheck** "Skip Previously Scraped Agents"
2. Run search
3. Will include previously scraped agents

### To Clear History

1. Click **üóëÔ∏è Clear Scraping History** button at bottom of sidebar
2. Confirmation message appears
3. All history cleared - next search includes all agents

---

## Where Is History Stored?

**Location:** `~/.agentharvest_history.json`

**Full paths by OS:**
- **macOS/Linux:** `/Users/your_name/.agentharvest_history.json`
- **Windows:** `C:\Users\your_name\.agentharvest_history.json`

**File format:**
```json
{
  "agent_ids": [
    "agent_id_1",
    "agent_id_2",
    "agent_id_3"
  ],
  "last_updated": "2024-12-18T14:30:00",
  "total_count": 3
}
```

**Safe to delete manually** - Just delete the file and history is cleared.

---

## Common Scenarios

### Scenario 1: Building a Database Over Time

**Goal:** Scrape 10,000 California agents over 2 weeks

**Approach:**
```python
# Run daily with duplicate prevention enabled (default)
agents = scrape_agent(state="California", limit=500, fetch_profiles=True)
agents.to_csv(f"agents_{datetime.now().date()}.csv", index=False)
```

**Result:**
- Day 1: 500 new agents
- Day 2: 500 new agents (no duplicates from Day 1)
- Day 3: 500 new agents (no duplicates from Days 1-2)
- ...
- Day 20: 10,000 total unique agents

### Scenario 2: Re-Scraping for Updates

**Goal:** Update contact info for existing agents

**Approach:**
```python
# Disable duplicate prevention
agents = scrape_agent(
    state="Nevada",
    limit=1000,
    fetch_profiles=True,
    skip_scraped=False  # Allow re-scraping
)
```

**Result:** Gets all 1000 agents even if scraped before

### Scenario 3: Testing

**Goal:** Test scraper with same search multiple times

**Approach:**
```python
from agentharvest import clear_scraping_history, scrape_agent

# Clear history before each test
clear_scraping_history()

# Run test
agents = scrape_agent(state="Nevada", limit=10)
```

---

## Performance Impact

**Minimal:** History checking is very fast
- **Load history:** < 100ms for 10,000 IDs
- **Filter duplicates:** < 50ms for 1,000 agents
- **Save history:** < 100ms

**Recommendation:** Keep enabled (default) for all production use.

---

## Troubleshooting

### "All agents were already scraped"

**Problem:** You're trying to scrape the same location again

**Solutions:**
1. Search a different location (different state/city)
2. Disable duplicate prevention: `skip_scraped=False`
3. Clear history: `clear_scraping_history()`

### History file not working

**Check:**
```python
from agentharvest.core.scrapers.zillow.history import ScrapingHistory

history = ScrapingHistory()
print(f"History file: {history.get_history_file()}")
print(f"Total scraped: {history.get_count()}")
```

**Reset:**
```python
from agentharvest import clear_scraping_history
clear_scraping_history()
```

---

## Best Practices

### ‚úÖ DO

1. **Keep duplicate prevention enabled** (default) for production
2. **Let history accumulate** over time for maximum coverage
3. **Clear history** only when starting a completely new project
4. **Export results to CSV** after each session (history doesn't backup data)

### ‚ùå DON'T

1. **Don't disable** duplicate prevention unless specifically needed
2. **Don't manually edit** the history JSON file (corruption risk)
3. **Don't rely on history** as your data backup (it only stores IDs)

---

## Summary

**Default behavior (Recommended):**
- ‚úÖ Duplicate prevention: **ENABLED**
- ‚úÖ History tracking: **AUTOMATIC**
- ‚úÖ Zero configuration needed
- ‚úÖ Works perfectly for:
  - Daily scraping
  - Building databases over time
  - Multi-session projects
  - Avoiding duplicate contacts

**Your partner can use the scraper daily without any duplicate concerns!**
